{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Malhar-Girgaonkar/ArtClassification/blob/main/Art_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QThPwF2mlGwj"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade pip\n",
        "#!pip install --upgrade setuptools\n",
        "#!pip install --upgrade wheel\n",
        "#!pip install  opencv-python matplotlib\n",
        "#!pip install tensorflow-gpu\n",
        "#!pip install --upgrade wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xLavhUZiYA4"
      },
      "outputs": [],
      "source": [
        "#Import modules\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOc_-nXViHWn"
      },
      "outputs": [],
      "source": [
        "#extract zip files it took me 8m 20s\n",
        "\n",
        "\n",
        "# Replace 'path_to_zip_file' and 'extracted_folder' with your actual file and folder names\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/Art Classifier Dataset/archive.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/extracted_folder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdTW431Baypa"
      },
      "outputs": [],
      "source": [
        "#Code to delete dataset\n",
        "\n",
        "folder_to_delete = '/content/extracted_folder'\n",
        "shutil.rmtree(folder_to_delete)\n",
        "try:\n",
        "    os.listdir(folder_to_delete)\n",
        "    print(f\"The folder {folder_to_delete} was not deleted.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"The folder {folder_to_delete} has been successfully deleted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2YXyUzahXF9",
        "outputId": "50553304-6908-485c-df28-9236ec87eaaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#mounting google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3kEhmqrYPeL"
      },
      "source": [
        "Creating directories and Setting dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgJBhAcqtxt1"
      },
      "outputs": [],
      "source": [
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "# Get all the paths\n",
        "data_dir_list = os.listdir('/content/drive/MyDrive/Art Classifier Dataset/Datasets')\n",
        "print(data_dir_list)\n",
        "path, dirs, files = next(os.walk(\"/content/drive/MyDrive/Art Classifier Dataset/Datasets\"))\n",
        "file_count = len(files)\n",
        "#print(file_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jxGMz_kvFAU"
      },
      "outputs": [],
      "source": [
        "# Make new base directory\n",
        "original_dataset_dir = '/content/drive/MyDrive/Art Classifier Dataset/Datasets'\n",
        "base_dir = '/content/drive/MyDrive/Art Classifier Dataset/Dataused'\n",
        "os.mkdir(base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o3K6-D3vtmD"
      },
      "outputs": [],
      "source": [
        "#create two folders (train and validation)\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.mkdir(validation_dir)\n",
        "\n",
        "#Under train folder create 13 folders\n",
        "# ['Western_Medieval', 'Renaissance', 'Rococo', 'Realism', 'Expressionism',\n",
        "#'Japanese_Art', 'Symbolism', 'Neoclassicism', 'Primitivism', 'Romanticism',\n",
        "# 'Academic_Art', 'Baroque', 'Art_Nouveau']\n",
        "\n",
        "train_Academic_Art_dir = os.path.join(train_dir, 'Academic_Art')\n",
        "os.mkdir(train_Academic_Art_dir)\n",
        "\n",
        "train_Art_Nouveau_dir = os.path.join(train_dir, 'Art_Nouveau')\n",
        "os.mkdir(train_Art_Nouveau_dir)\n",
        "\n",
        "train_Baroque_dir = os.path.join(train_dir, 'Baroque')\n",
        "os.mkdir(train_Baroque_dir)\n",
        "\n",
        "train_Expressionism_dir = os.path.join(train_dir, 'Expressionism')\n",
        "os.mkdir(train_Expressionism_dir)\n",
        "\n",
        "train_Japanese_Art_dir = os.path.join(train_dir, 'Japanese_Art')\n",
        "os.mkdir(train_Japanese_Art_dir)\n",
        "\n",
        "train_Neoclassicism_dir = os.path.join(train_dir, 'Neoclassicism')\n",
        "os.mkdir(train_Neoclassicism_dir)\n",
        "\n",
        "train_Primitivism_dir = os.path.join(train_dir, 'Primitivism')\n",
        "os.mkdir(train_Primitivism_dir)\n",
        "\n",
        "train_Realism_dir = os.path.join(train_dir, 'Realism')\n",
        "os.mkdir(train_Realism_dir)\n",
        "\n",
        "train_Renaissance_dir = os.path.join(train_dir, 'Renaissance')\n",
        "os.mkdir(train_Renaissance_dir)\n",
        "\n",
        "train_Rococo_dir = os.path.join(train_dir, 'Rococo')\n",
        "os.mkdir(train_Rococo_dir)\n",
        "\n",
        "train_Romanticism_dir = os.path.join(train_dir, 'Romanticism')\n",
        "os.mkdir(train_Romanticism_dir)\n",
        "\n",
        "train_Symbolism_dir = os.path.join(train_dir, 'Symbolism')\n",
        "os.mkdir(train_Symbolism_dir)\n",
        "\n",
        "train_Western_Medieval_dir = os.path.join(train_dir, 'Western_Medieval')\n",
        "os.mkdir(train_Western_Medieval_dir)\n",
        "\n",
        "#Under validation folder create 13 folders\n",
        "# ['Western_Medieval', 'Renaissance', 'Rococo', 'Realism', 'Expressionism',\n",
        "#'Japanese_Art', 'Symbolism', 'Neoclassicism', 'Primitivism', 'Romanticism',\n",
        "# 'Academic_Art', 'Baroque', 'Art_Nouveau']\n",
        "\n",
        "validation_Academic_Art_dir = os.path.join(validation_dir, 'Academic_Art')\n",
        "os.mkdir(validation_Academic_Art_dir)\n",
        "\n",
        "validation_Art_Nouveau_dir = os.path.join(validation_dir, 'Art_Nouveau')\n",
        "os.mkdir(validation_Art_Nouveau_dir)\n",
        "\n",
        "validation_Baroque_dir = os.path.join(validation_dir, 'Baroque')\n",
        "os.mkdir(validation_Baroque_dir)\n",
        "\n",
        "validation_Expressionism_dir = os.path.join(validation_dir, 'Expressionism')\n",
        "os.mkdir(validation_Expressionism_dir)\n",
        "\n",
        "validation_Japanese_Art_dir = os.path.join(validation_dir, 'Japanese_Art')\n",
        "os.mkdir(validation_Japanese_Art_dir)\n",
        "\n",
        "validation_Neoclassicism_dir = os.path.join(validation_dir, 'Neoclassicism')\n",
        "os.mkdir(validation_Neoclassicism_dir)\n",
        "\n",
        "validation_Primitivism_dir = os.path.join(validation_dir, 'Primitivism')\n",
        "os.mkdir(validation_Primitivism_dir)\n",
        "\n",
        "validation_Realism_dir = os.path.join(validation_dir, 'Realism')\n",
        "os.mkdir(validation_Realism_dir )\n",
        "\n",
        "validation_Renaissance_dir = os.path.join(validation_dir, 'Renaissance')\n",
        "os.mkdir(validation_Renaissance_dir)\n",
        "\n",
        "validation_Rococo_dir = os.path.join(validation_dir, 'Rococo')\n",
        "os.mkdir(validation_Rococo_dir)\n",
        "\n",
        "train_Romanticism_dir = os.path.join(validation_dir, 'Romanticism')\n",
        "os.mkdir(train_Romanticism_dir)\n",
        "\n",
        "train_Symbolism_dir = os.path.join(validation_dir, 'Symbolism')\n",
        "os.mkdir(train_Symbolism_dir)\n",
        "\n",
        "train_Western_Medieval_dir = os.path.join(validation_dir, 'Western_Medieval')\n",
        "os.mkdir(train_Western_Medieval_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H38M0l1_4eM7"
      },
      "outputs": [],
      "source": [
        "#Spliting dataset into training testing validation\n",
        "\n",
        "def split_data(SOURCE, TRAINING, VALIDATION, SPLIT_SIZE):\n",
        "    files = []\n",
        "    for filename in os.listdir(SOURCE):\n",
        "        file = SOURCE + filename\n",
        "        if os.path.getsize(file) > 0:\n",
        "            files.append(filename)\n",
        "        else:\n",
        "            print(filename + \" is zero length, so ignoring.\")\n",
        "\n",
        "    training_length = int(len(files) * SPLIT_SIZE)\n",
        "    valid_length = int(len(files) - training_length)\n",
        "    shuffled_set = random.sample(files, len(files))\n",
        "    training_set = shuffled_set[0:training_length]\n",
        "    valid_set = shuffled_set[training_length:]\n",
        "\n",
        "    for filename in training_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = TRAINING + filename\n",
        "        shutil.copyfile(this_file, destination)\n",
        "\n",
        "    for filename in valid_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = VALIDATION + filename\n",
        "        shutil.copyfile(this_file, destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6mPNLxs9JM6"
      },
      "outputs": [],
      "source": [
        "#List directories to be used in spliting\n",
        "\n",
        "Academic_Art_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Academic_Art/'\n",
        "TRAINING_Academic_Art_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/Academic_Art/'\n",
        "VALID_Academic_Art_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/Academic_Art/'\n",
        "\n",
        "Art_Nouveau_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Art_Nouveau/'\n",
        "TRAINING_Art_Nouveau_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/Art_Nouveau/'\n",
        "VALID_Art_Nouveau_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/Art_Nouveau/'\n",
        "\n",
        "Baroque_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Baroque/'\n",
        "TRAINING_Baroque_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/Baroque/'\n",
        "VALID_Baroque_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/Baroque/'\n",
        "\n",
        "Expressionism_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Expressionism/'\n",
        "TRAINING_Expressionism_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/Expressionism/'\n",
        "VALID_Expressionism_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/Expressionism/'\n",
        "\n",
        "Japanese_Art_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Japanese_Art/'\n",
        "TRAINING_Japanese_Art_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/Japanese_Art/'\n",
        "VALID_Japanese_Art_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/Japanese_Art/'\n",
        "\n",
        "Neoclassicism_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Neoclassicism/'\n",
        "TRAINING_Neoclassicism_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/Neoclassicism/'\n",
        "VALID_Neoclassicism_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/Neoclassicism/'\n",
        "\n",
        "Primitivism_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Primitivism/'\n",
        "TRAINING_Primitivism_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/Primitivism/'\n",
        "VALID_Primitivism_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/Primitivism/'\n",
        "\n",
        "Realism_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Realism/'\n",
        "TRAINING_Realism_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/Realism/'\n",
        "VALID_Realism_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/Realism/'\n",
        "\n",
        "Renaissance_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Renaissance/'\n",
        "TRAINING_Renaissance_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/Renaissance/'\n",
        "VALID_Renaissance_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/Renaissance/'\n",
        "\n",
        "Rococo_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Rococo/'\n",
        "TRAINING_Rococo_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/Rococo/'\n",
        "VALID_Rococo_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/Rococo/'\n",
        "\n",
        "Romanticism_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Romanticism/'\n",
        "TRAINING_Romanticism_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/Romanticism/'\n",
        "VALID_Romanticism_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/Romanticism/'\n",
        "\n",
        "Symbolism_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Symbolism/'\n",
        "TRAINING_Symbolism_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/Symbolism/'\n",
        "VALID_Symbolism_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/Symbolism/'\n",
        "\n",
        "Western_Medieval_SOURCE_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Datasets/Western_Medieval/'\n",
        "TRAINING_Western_Medieval_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/Western_Medieval/'\n",
        "VALID_Western_Medieval_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/Western_Medieval/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vL1SEhl47UfL"
      },
      "outputs": [],
      "source": [
        "#Seting split of 80,20 for training,validation of size 21.6gb,5.4gb approx\n",
        "#it took me 48m 1s to run this script\n",
        "\n",
        "split_size = .85\n",
        "\n",
        "split_data(Academic_Art_SOURCE_DIR, TRAINING_Academic_Art_DIR, VALID_Academic_Art_DIR , split_size)\n",
        "\n",
        "split_data(Art_Nouveau_SOURCE_DIR, TRAINING_Art_Nouveau_DIR, VALID_Art_Nouveau_DIR , split_size)\n",
        "\n",
        "split_data(Baroque_SOURCE_DIR, TRAINING_Baroque_DIR, VALID_Baroque_DIR , split_size)\n",
        "\n",
        "split_data(Expressionism_SOURCE_DIR, TRAINING_Expressionism_DIR, VALID_Expressionism_DIR , split_size)\n",
        "\n",
        "split_data(Japanese_Art_SOURCE_DIR, TRAINING_Japanese_Art_DIR, VALID_Japanese_Art_DIR , split_size)\n",
        "\n",
        "split_data(Neoclassicism_SOURCE_DIR, TRAINING_Neoclassicism_DIR, VALID_Neoclassicism_DIR , split_size)\n",
        "\n",
        "split_data(Primitivism_SOURCE_DIR, TRAINING_Primitivism_DIR, VALID_Primitivism_DIR , split_size)\n",
        "\n",
        "split_data(Realism_SOURCE_DIR, TRAINING_Realism_DIR, VALID_Realism_DIR , split_size)\n",
        "\n",
        "split_data(Renaissance_SOURCE_DIR, TRAINING_Renaissance_DIR, VALID_Renaissance_DIR , split_size)\n",
        "\n",
        "split_data(Rococo_SOURCE_DIR, TRAINING_Rococo_DIR, VALID_Rococo_DIR , split_size)\n",
        "\n",
        "split_data(Romanticism_SOURCE_DIR, TRAINING_Romanticism_DIR, VALID_Romanticism_DIR , split_size)\n",
        "\n",
        "split_data(Symbolism_SOURCE_DIR, TRAINING_Symbolism_DIR, VALID_Symbolism_DIR , split_size)\n",
        "\n",
        "split_data(Western_Medieval_SOURCE_DIR, TRAINING_Western_Medieval_DIR, VALID_Western_Medieval_DIR , split_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuSzmTjlU6hU"
      },
      "outputs": [],
      "source": [
        "#Looking at data in training part\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.image import imread\n",
        "import pathlib\n",
        "\n",
        "image_folder = ['Western_Medieval', 'Renaissance', 'Rococo', 'Realism', 'Expressionism', 'Japanese_Art', 'Symbolism', 'Neoclassicism', 'Primitivism', 'Romanticism', 'Academic_Art', 'Baroque', 'Art_Nouveau']\n",
        "\n",
        "nimgs = {}\n",
        "for i in image_folder:\n",
        "    nimages = len(os.listdir('/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/'+i+'/'))\n",
        "    nimgs[i]=nimages\n",
        "plt.figure(figsize=(18, 6))\n",
        "plt.bar(range(len(nimgs)), list(nimgs.values()), align='center')\n",
        "plt.xticks(range(len(nimgs)), list(nimgs.keys()))\n",
        "plt.title('Distribution of different classes in Training Dataset')\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "#print no of images in each category\n",
        "for i in image_folder:\n",
        "    print('Training {} images are: '.format(i)+str(len(os.listdir('/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/'+i+'/'))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_2NVR8AWAli"
      },
      "outputs": [],
      "source": [
        "#Looking data in validation part\n",
        "nimgs = {}\n",
        "for i in image_folder:\n",
        "    nimages = len(os.listdir('/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/'+i+'/'))\n",
        "    nimgs[i]=nimages\n",
        "plt.figure(figsize=(18, 6))\n",
        "plt.bar(range(len(nimgs)), list(nimgs.values()), align='center')\n",
        "plt.xticks(range(len(nimgs)), list(nimgs.keys()))\n",
        "plt.title('Distribution of different classes in Validation Dataset')\n",
        "plt.show()\n",
        "print(\"\\n\\n\")\n",
        "#print no of images in each category\n",
        "for i in image_folder:\n",
        "    print('Testing {} images are: '.format(i)+str(len(os.listdir('/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/'+i+'/'))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwdlOeXAZziH"
      },
      "source": [
        "Removing Dodgy Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOMovMzmZ5It"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import imghdr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kapAXdUdZ9Mf"
      },
      "outputs": [],
      "source": [
        "data_dir = 'Your_Data_Path'\n",
        "image_exts = ['jpeg','jpg', 'bmp', 'png']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWL3Gu1ZaH7P"
      },
      "outputs": [],
      "source": [
        "for image_class in os.listdir(data_dir):\n",
        "    for image in os.listdir(os.path.join(data_dir, image_class)):\n",
        "        image_path = os.path.join(data_dir, image_class, image)\n",
        "        try:\n",
        "            img = cv2.imread(image_path)\n",
        "            tip = imghdr.what(image_path)\n",
        "            if tip not in image_exts:\n",
        "                print('Image not in ext list {}'.format(image_path))\n",
        "                os.remove(image_path)\n",
        "        except Exception as e:\n",
        "            print('Issue with image {}'.format(image_path))\n",
        "            # os.remove(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3w7CrPEYFYQ"
      },
      "source": [
        "Deep Learning Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZkBC8mnYeeZ"
      },
      "outputs": [],
      "source": [
        "#import deep learning based modules\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0yS0BIxAzzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45efec84-b5b2-4f19-d72c-b07dc3cb96d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#Check if gpu is allocated\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "len(gpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRDbWPpiAMGY"
      },
      "outputs": [],
      "source": [
        "#Limit gpu usage to reasonable level and avoid OOM errors\n",
        "\n",
        "# List available physical GPUs\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "\n",
        "# Set memory growth for each GPU\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Riz47ijyd729"
      },
      "outputs": [],
      "source": [
        "#Setting image height and width and batch size\n",
        "img_width=256; img_height=256\n",
        "batch_size=512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBpAlw1Jenxa"
      },
      "outputs": [],
      "source": [
        "#set training directory path and create a pipeline as datagen\n",
        "TRAINING_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/train/'\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale = 1/255.0,\n",
        "                                   rotation_range=30,\n",
        "                                   zoom_range=0.4,\n",
        "                                   horizontal_flip=True)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(img_height, img_width)\n",
        "                                                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D9NYdo90fHK0",
        "outputId": "4416a3c3-bbe7-44e2-9e79-631b44f3ec7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5977 images belonging to 13 classes.\n"
          ]
        }
      ],
      "source": [
        "#Set validation directory path and pipeline\n",
        "VALIDATION_DIR = '/content/drive/MyDrive/Art Classifier Dataset/Dataused/validation/'\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=batch_size,\n",
        "                                                              class_mode='categorical',\n",
        "                                                              target_size=(img_height, img_width)\n",
        "                                                             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flh2sn9ChKoY"
      },
      "outputs": [],
      "source": [
        "#Preventing overfitting using Earlystopping and saving best model using ModelCheckpoint\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='auto')\n",
        "# autosave best Model in location :\n",
        "best_model_file = '/content/drive/MyDrive/Art Classifier Dataset/Best_Models/CNN_aug_best_weights.h5'\n",
        "best_model = ModelCheckpoint(best_model_file, monitor='val_acc', verbose = 1, save_best_only = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJXkt-bci0f1"
      },
      "outputs": [],
      "source": [
        "#********************Convulation Neural Network Training starts here********************\n",
        "#CNN structure NOTE:last dense layer value must match number of classes,ie=13\n",
        "model = Sequential([\n",
        "    Conv2D(16, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)), MaxPooling2D(2, 2),\n",
        "    Conv2D(32, (3, 3), activation='relu'), MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(256, (3, 3), activation='relu'),\n",
        "    Conv2D(256, (3, 3), activation='relu'),\n",
        "    Conv2D(256, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(13, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSbzUWn1j2rt"
      },
      "outputs": [],
      "source": [
        "#Compiling model\n",
        "model.compile(optimizer='Adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics =['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "tzAAta5ekrzw",
        "outputId": "c335efa4-7835-49ef-fba0-ce3a04ad99c2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-426900b876a2>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#This part is actually training of model and takes long time consider before running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#here we start with moderate epoch of 50 or 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(train_generator,\n\u001b[0m\u001b[1;32m      4\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "#This part is actually training of model and takes long time consider before running\n",
        "#here we start with moderate epoch of 50 or 100\n",
        "history = model.fit(train_generator,\n",
        "                              epochs=20,\n",
        "                              verbose=1,\n",
        "                              validation_data=validation_generator,\n",
        "                              callbacks = [early_stopping,best_model]\n",
        "                              )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "18VjCFDThvzWknwq0gjQdQpvfzSo2Ry1B",
      "authorship_tag": "ABX9TyPRznJzwy3XGxX9Rk+aDOTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}